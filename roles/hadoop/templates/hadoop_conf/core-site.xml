<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<!--
  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License. See accompanying LICENSE file.
-->

{{ ansible_managed | comment('xml') }}

<!-- Put site-specific property overrides in this file. -->

<configuration>

    <property>
        <name>fs.defaultFS</name>
        {% if hdfs_high_availability %}
            <value>hdfs://{{ hdfs_name }}</value>
        {% else %}
            <value>hdfs://{{ hdfs_nonha_primary_namenode }}</value>
        {% endif %}
        <description>The name of the default file system. A URI whose
            scheme and authority determine the FileSystem implementation. The
            uri's scheme determines the config property (fs.SCHEME.impl) naming
            the FileSystem implementation class. The uri's authority is used to
            determine the host, port, etc. for a filesystem.
        </description>
    </property>

    <property>
        <name>hadoop.tmp.dir</name>
        <value>{{ hadoop_tmp }}</value>
        <description>A base for other temporary directories.</description>
    </property>
    <property>
        <name>io.file.buffer.size</name>
        <value>131072</value>
        <description>The size of buffer for use in sequence files.
            The size of this buffer should probably be a multiple of hardware
            page size (4096 on Intel x86), and it determines how much data is
            buffered during read and write operations.
        </description>
    </property>


    <property>
        <name>fs.trash.checkpoint.interval</name>
        <value>360</value>
        <description>Number of minutes between trash checkpoints.
            Should be smaller or equal to fs.trash.interval. If zero,
            the value is set to the value of fs.trash.interval.
            Every time the checkpointer runs it creates a new checkpoint
            out of current and removes checkpoints created more than
            fs.trash.interval minutes ago.
        </description>
    </property>
    <property>
        <name>io.compression.codecs</name>
        <value>
            org.apache.hadoop.io.compress.GzipCodec,org.apache.hadoop.io.compress.DefaultCodec,org.apache.hadoop.io.compress.SnappyCodec
        </value>
        <description>A comma-separated list of the compression codec classes that can
            be used for compression/decompression. In addition to any classes specified
            with this property (which take precedence), codec classes on the classpath
            are discovered using a Java ServiceLoader.
        </description>
    </property>

    <property>
        <name>hadoop.zk.address</name>
        <value>{{ zookeeper_quorum }}</value>
        <description>Comma separated list of Host:Port pairs. Each corresponds to a ZooKeeper server (e.g.
            "127.0.0.1:3000,127.0.0.1:3001,127.0.0.1:3002") to be used by the RM for storing RM state.
        </description>
    </property>

    {% if yarn_high_availability %}

        <!-- YARN registry -->
        <property>
            <name>hadoop.registry.zk.quorum</name>
            <value>{{ zookeeper_quorum }}</value>
            <description>
                List of hostname:port pairs defining the
                zookeeper quorum binding for the registry
            </description>
        </property>
        <property>
            <name>hadoop.registry.client.auth</name>
            <value>kerberos</value>
        </property>

        <property>
            <name>hadoop.registry.secure</name>
            <value>true</value>
            <description>
                Key to set if the registry is secure. Turning it on
                changes the permissions policy from "open access"
                to restrictions on kerberos with the option of
                a user adding one or more auth key pairs down their
                own tree.
            </description>
        </property>
        <property>
            <name>hadoop.registry.system.acls</name>
            <value>
                sasl:{{ yarn_user.name }}, sasl:{{ mapreduce_user.name }}, sasl:{{ hadoop_group.name }}, sasl:{{ hdfs_user.name }}, sasl:{{ yarn_resourcemanager_service_name }}</value>
        </property>
        <property>
            <name>hadoop.registry.kerberos.realm</name>
            <value>{{ ipaserver_realm }}</value>
            <description>
                The kerberos realm: used to set the realm of
                system principals which do not declare their realm,
                and any other accounts that need the value.

                If empty, the default realm of the running process
                is used.

                If neither are known and the realm is needed, then the registry
                service/client will fail.
            </description>
        </property>
    {% endif %}

    {% if hdfs_high_availability %}

        <property>
            <name>ha.failover-controller.active-standby-elector.zk.op.retries</name>
            <value>120</value>
            <description>
                The number of zookeeper operation retry times in ActiveStandbyElector
            </description>
        </property>
        <property>
            <name>ha.zookeeper.acl</name>
            <value>sasl:{{ hdfs_namenode_service_name }}:rwcda</value>
            <description>
                A comma-separated list of ZooKeeper ACLs to apply to the znodes
                used by automatic failover. These ACLs are specified in the same
                format as used by the ZooKeeper CLI.

                If the ACL itself contains secrets, you may instead specify a
                path to a file, prefixed with the '@' symbol, and the value of
                this configuration will be loaded from within.
            </description>
        </property>
        <property>
            <name>ha.zookeeper.quorum</name>
            <value>{{ zookeeper_quorum }}</value>
            <description>
                A list of ZooKeeper server addresses, separated by commas, that are
                to be used by the ZKFailoverController in automatic failover.
            </description>
        </property>
    {% endif %}

    {% if kerberos_authentication %}
        <property>
            <name>hadoop.http.authentication.cookie.domain</name>
            <value>{{ ipaserver_domain }}</value>
            <description>
                The domain to use for the HTTP cookie that stores the authentication token.
                In order to authentiation to work correctly across all Hadoop nodes web-consoles
                the domain must be correctly set.
                IMPORTANT: when using IP addresses, browsers ignore cookies with domain settings.
                For this setting to work properly all nodes in the cluster must be configured
                to generate URLs with hostname.domain names on it.
            </description>
        </property>
        <property>
            <name>hadoop.http.authentication.kerberos.keytab</name>
            <value>{{ keytab_dir }}/spnego.service.keytab</value>
            <description>
                Location of the keytab file with the credentials for the principal.
                Referring to the same keytab file Oozie uses for its Kerberos credentials for Hadoop.
            </description>
        </property>
        <property>
            <name>hadoop.http.authentication.kerberos.principal</name>
            <value>HTTP/_HOST@{{ ipaserver_realm }}</value>
            <description>
                Indicates the Kerberos principal to be used for HTTP endpoint.
                The principal MUST start with 'HTTP/' as per Kerberos HTTP SPNEGO specification.
            </description>
        </property>
        <property>
            <name>hadoop.http.authentication.signature.secret.file</name>
            <value>/etc/security/http_secret</value>
            <description>
                The signature secret for signing the authentication tokens.
                The same secret should be used for RM/NM/NN/DN configurations.
            </description>
        </property>
        <property>
            <name>hadoop.http.authentication.simple.anonymous.allowed</name>
            <value>false</value>
            <description>
                Indicates if anonymous requests are allowed when using 'simple' authentication.
            </description>
        </property>
        <property>
            <name>hadoop.http.authentication.type</name>
            <value>kerberos</value>
            <description>
                Defines authentication used for Oozie HTTP endpoint.
                Supported values are: simple | kerberos | #AUTHENTICATION_HANDLER_CLASSNAME#
            </description>
        </property>
        <property>
            <name>hadoop.http.filter.initializers</name>
            <value>org.apache.hadoop.security.AuthenticationFilterInitializer</value>
            <description>A comma separated list of class names. Each class in the list
                must extend org.apache.hadoop.http.FilterInitializer. The corresponding
                Filter will be initialized. Then, the Filter will be applied to all user
                facing jsp and servlet web pages. The ordering of the list defines the
                ordering of the filters.
            </description>
        </property>
        <property>
            <name>hadoop.security.auth_to_local</name>
            <value>
                RULE:[1:$1@$0](.*@{{ ipaserver_realm }})s/@.*//
                RULE:[2:$1@$0]({{ hdfs_datanode_service_name }}@{{ ipaserver_realm }})s/.*/{{ hdfs_user.name }}/
                RULE:[2:$1@$0]({{ mapreduce_historyserver_service_name }}@{{ ipaserver_realm }})s/.*/{{ mapreduce_user.name }}/
                RULE:[2:$1@$0]({{ hdfs_journalnode_service_name }}@{{ ipaserver_realm }})s/.*/{{ hdfs_user.name }}/
                RULE:[2:$1@$0]({{ yarn_nodemanager_service_name }}@{{ ipaserver_realm }})s/.*/{{ yarn_user.name }}/
                RULE:[2:$1@$0]({{ hdfs_namenode_service_name }}@{{ ipaserver_realm }})s/.*/{{ hdfs_user.name }}/
                RULE:[2:$1@$0]({{ hdfs_secondary_namenode_service_name }}@{{ ipaserver_realm }})s/.*/{{ hdfs_user.name }}/
                RULE:[2:$1@$0]({{ yarn_resourcemanager_service_name }}@{{ ipaserver_realm }})s/.*/{{ yarn_user.name }}/
                RULE:[2:$1@$0]({{ yarn_timelineserver_service_name }}@{{ ipaserver_realm }})s/.*/{{ yarn_user.name }}/
                RULE:[2:$1@$0]({{ spark_history_server_service_name }}@{{ ipaserver_realm }})s/.*/{{ spark_user.name }}/
                DEFAULT
            </value>
            <description>Maps kerberos principals to local user names</description>

        </property>
        <property>
            <name>hadoop.security.authentication</name>
            <value>kerberos</value>
            <description>Possible values are simple (no authentication), and kerberos
            </description>
        </property>
        <property>
            <name>hadoop.security.authorization</name>
            <value>true</value>
            <description>Is service-level authorization enabled?</description>

        </property>

    {% endif %}


    <property>
        <name>hadoop.ssl.client.conf</name>
        <value>ssl-client.xml</value>
        <description>
            Resource file from which ssl client keystore information will be extracted
            This file is looked up in the classpath, typically it should be in Hadoop
            conf/ directory.
        </description>
    </property>
    <property>
        <name>hadoop.ssl.server.conf</name>
        <value>ssl-server.xml</value>
        <description>
            Resource file from which ssl server keystore information will be extracted.
            This file is looked up in the classpath, typically it should be in Hadoop
            conf/ directory.
        </description>
    </property>

    <property>
        <name>hadoop.proxyuser.HTTP.groups</name>
        <value>{{ users_group }}</value>
    </property>

    <property>
        <name>spark.authenticate</name>
        <value>true</value>
        <description>Prevents spark from breaking with "java.lang.IllegalArgumentException: requirement failed: A secret
            key must be specified via the spark.authenticate.secret config."
        </description>
    </property>


</configuration>
