<?xml version="1.0"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<!--
  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at
    http://www.apache.org/licenses/LICENSE-2.0
  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License. See accompanying LICENSE file.
-->

{{ ansible_managed | comment('xml') }}

<configuration>


    <property>
        <name>spark.authenticate</name>
        <value>true</value>
    </property>

    <!-- ACLs -->
    <property>
        <name>yarn.acl.enable</name>
        <value>true</value>
        <description>Are acls enabled.</description>
        <default>false</default>
    </property>

    <property>
        <name>yarn.admin.acl</name>
        <value>{{ yarn_user.name }} {{ subadmins_group.name }}</value>
        <description>ACL of who can be admin of the YARN cluster.</description>
        <default>*</default>
    </property>


    <property>
        <name>yarn.application.classpath</name>
        <value>{{ hadoop_config_path }},{{ hadoop_path }}/hadoop-{{ hadoop_version }}/lib/*</value>
        <description>
            CLASSPATH for YARN applications. A comma-separated list
            of CLASSPATH entries. When this value is empty, the following default
            CLASSPATH for YARN applications would be used.
            For Linux:
            $HADOOP_CONF_DIR,
            $HADOOP_COMMON_HOME/share/hadoop/common/*,
            $HADOOP_COMMON_HOME/share/hadoop/common/lib/*,
            $HADOOP_HDFS_HOME/share/hadoop/hdfs/*,
            $HADOOP_HDFS_HOME/share/hadoop/hdfs/lib/*,
            $HADOOP_YARN_HOME/share/hadoop/yarn/*,
            $HADOOP_YARN_HOME/share/hadoop/yarn/lib/*
            For Windows:
            %HADOOP_CONF_DIR%,
            %HADOOP_COMMON_HOME%/share/hadoop/common/*,
            %HADOOP_COMMON_HOME%/share/hadoop/common/lib/*,
            %HADOOP_HDFS_HOME%/share/hadoop/hdfs/*,
            %HADOOP_HDFS_HOME%/share/hadoop/hdfs/lib/*,
            %HADOOP_YARN_HOME%/share/hadoop/yarn/*,
            %HADOOP_YARN_HOME%/share/hadoop/yarn/lib/*
        </description>
        <default></default>
    </property>


    <!-- SSL -->
    <property>
        <name>yarn.http.policy</name>
        <value>HTTPS_ONLY</value>
        <description>
            This configures the HTTP endpoint for YARN Daemons.The following
            values are supported:
            - HTTP_ONLY : Service is provided only on http
            - HTTPS_ONLY : Service is provided only on https
        </description>
        <default>HTTP_ONLY</default>
    </property>


    <!-- Log aggregation -->
    <property>
        <name>yarn.log-aggregation-enable</name>
        <value>true</value>
        <description>Whether to enable log aggregation. Log aggregation collects
            each container's logs and moves these logs onto a file-system, for e.g.
            HDFS, after the application completes. Users can configure the
            "yarn.nodemanager.remote-app-log-dir" and
            "yarn.nodemanager.remote-app-log-dir-suffix" properties to determine
            where these logs are moved to. Users can access the logs via the
            Application Timeline Server.
        </description>
        <default>false</default>
    </property>
    <property>
        <name>yarn.nodemanager.remote-app-log-dir</name>
        <value>{{ yarn_nodemanager_remote_app_log_dir }}</value>
        <description>Where to aggregate logs to.</description>
    </property>
    <property>
        <name>yarn.log-aggregation.file-formats</name>
        <value>IndexedFormat,TFile</value>
        <description>Specify which log file controllers we will support. The first
            file controller we add will be used to write the aggregated logs.
            This comma separated configuration will work with the configuration:
            yarn.log-aggregation.file-controller.%s.class which defines the supported
            file controller's class. By default, the TFile controller would be used.
            The user could override this configuration by adding more file controllers.
            To support back-ward compatibility, make sure that we always
            add TFile file controller.
        </description>
        <default>TFile</default>
    </property>
    <property>
        <name>yarn.log-aggregation.file-controller.IndexedFormat.class</name>
        <value>org.apache.hadoop.yarn.logaggregation.filecontroller.ifile.LogAggregationIndexedFileController</value>
    </property>
    <property>
        <name>yarn.log-aggregation.file-controller.TFile.class</name>
        <value>org.apache.hadoop.yarn.logaggregation.filecontroller.tfile.LogAggregationTFileController</value>
        <description>Class that supports TFile read and write operations.</description>
        <default>org.apache.hadoop.yarn.logaggregation.filecontroller.tfile.LogAggregationTFileController</default>
    </property>
    <property>
        <name>yarn.log-aggregation.retain-seconds</name>
        <value>2592000</value>
        <description>How long to keep aggregation logs before deleting them. -1 disables.
            Be careful set this too small and you will spam the name node.
        </description>
        <default>-1</default>
    </property>
    <property>
        <name>yarn.log.server.url</name>
        <value>http://{{ mapreduce_history_server_host }}:{{ mapreduce_history_server_https_port }}/jobhistory/logs
        </value>
        <description>
            URL for log aggregation server
        </description>
    </property>
    <property>
        <name>yarn.log.server.web-service.url</name>
        <value>http://{{ yarn_timelineserver }}:{{ yarn_timelineserver_http_port }}/ws/v1/applicationhistory</value>
        <description>
            URL for log aggregation server web service
        </description>
    </property>
    <property>
        <name>yarn.nodemanager.log-aggregation.compression-type</name>
        <value>gz</value>
        <description>T-file compression types used to compress aggregated logs.</description>
    </property>
    <property>
        <name>yarn.nodemanager.log-aggregation.num-log-files-per-app</name>
        <value>336</value>
        <description>Define how many aggregated log files per application per NM
            we can have in remote file system. By default, the total number of
            aggregated log files per application per NM is 30.
        </description>
    </property>
    <property>
        <name>yarn.nodemanager.log-aggregation.roll-monitoring-interval-seconds</name>
        <value>3600</value>
        <description>Defines how often NMs wake up to upload log files.
            The default value is -1. By default, the logs will be uploaded when
            the application is finished. By setting this configuration logs can
            be uploaded periodically while the application is running.
            The minimum positive accepted value can be configured by the setting
            "yarn.nodemanager.log-aggregation.roll-monitoring-interval-seconds.min".
        </description>
    </property>


    <!-- Node labels-->
    <property>
        <name>yarn.node-labels.enabled</name>
        <value>false</value>
        <description>
            Enable node labels feature
        </description>
    </property>


    <!-- Node manager -->


    <property>
        <name>yarn.nodemanager.recovery.enabled</name>
        <value>true</value>
        <description>Enable the node manager to recover after starting</description>
    </property>
    <property>
        <name>yarn.resourcemanager.nodemanagers.heartbeat-interval-ms</name>
        <value>5000</value>
        <description>The heart-beat interval in milliseconds for every NodeManager in the cluster.</description>
    </property>


    <!-- Node manager resources -->
    <property>
        <name>yarn.nodemanager.resource.cpu-vcores</name>
        <value>{{ yarn_nodemanager_available_cpu_cores }}</value>
        <description>Number of vcores that can be allocated
            for containers. This is used by the RM scheduler when allocating
            resources for containers. This is not used to limit the number of
            CPUs used by YARN containers. If it is set to -1 and
            yarn.nodemanager.resource.detect-hardware-capabilities is true, it is
            automatically determined from the hardware in case of Windows and Linux.
            In other cases, number of vcores is 8 by default.
        </description>
    </property>
    <property>
        <name>yarn.nodemanager.resource.memory-mb</name>
        <value>{{ yarn_nodemanager_available_memory }}</value>
        <description>Amount of physical memory, in MB, that can be allocated
            for containers. If set to -1 and
            yarn.nodemanager.resource.detect-hardware-capabilities is true, it is
            automatically calculated(in case of Windows and Linux).
            In other cases, the default is 8192MB.
        </description>
    </property>


    <!-- Node manager hosts and ports -->
    <property>
        <name>yarn.nodemanager.address</name>
        <value>0.0.0.0:{{ yarn_nodemanager_port }}</value>
        <description>The address of the container manager in the NM.</description>
        <value>${yarn.nodemanager.hostname}:0</value>
    </property>
    <property>
        <name>yarn.nodemanager.bind-host</name>
        <value>0.0.0.0</value>
        <description>
            The actual address the server will bind to. If this optional address is
            set, the RPC and webapp servers will bind to this address and the port specified in
            yarn.nodemanager.address and yarn.nodemanager.webapp.address, respectively. This is
            most useful for making NM listen to all interfaces by setting to 0.0.0.0.
        </description>
    </property>

    <property>
        <name>yarn.nodemanager.aux-services</name>
        <value>mapreduce_shuffle</value>
        <description>A comma separated list of services where service name should only
            contain a-zA-Z0-9_ and can not start with numbers
        </description>
        {#TODO spark shuffle here#}
    </property>
    <property>
        <name>yarn.nodemanager.aux-services.mapreduce_shuffle.class</name>
        <value>org.apache.hadoop.mapred.ShuffleHandler</value>
    </property>

    <!-- Node manager container executor -->
    <property>
        <name>yarn.nodemanager.container-executor.class</name>
        <value>org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor</value>
        <description>who will execute(launch) the containers.</description>
    </property>
    <property>
        <name>yarn.nodemanager.linux-container-executor.cgroups.strict-resource-usage</name>
        <value>false</value>
    </property>
    <property> <!--This should match field in container-executor.cfg -->
        <name>yarn.nodemanager.linux-container-executor.group</name>
        <value>{{ hadoop_group.name }}</value>
    </property>
    <property> <!--This should match field in container-executor.cfg -->
        <name>yarn.nodemanager.local-dirs</name>
        <value>{{ yarn_data_dirs_joined }}</value>
        <description>List of directories to store localized files in. An
            application's localized file directory will be found in:
            ${yarn.nodemanager.local-dirs}/usercache/${user}/appcache/application_${appid}.
            Individual containers' work directories, called container_${contid}, will
            be subdirectories of this.
        </description>
    </property>
    <property> <!--This should match field in container-executor.cfg -->
        <name>yarn.nodemanager.log-dirs</name>
        <value>{{ yarn_application_logs_dir }}</value>
        <description>
            Where to store container logs. An application's localized log directory
            will be found in ${yarn.nodemanager.log-dirs}/application_${appid}.
            Individual containers' log directories will be below this, in directories
            named container_{$contid}. Each container directory will contain the files
            stderr, stdin, and syslog generated by that container.
        </description>
    </property>
    <property>
        <name>yarn.nodemanager.container-monitor.interval-ms</name>
        <value>3000</value>
        <description>How often to monitor containers. If not set, the value for
            yarn.nodemanager.resource-monitor.interval-ms will be used.
            If 0 or negative, container monitoring is disabled.
        </description>
    </property>
    <property>
        <name>yarn.nodemanager.disk-health-checker.min-free-space-per-disk-mb</name>
        <value>1000</value>
        <description>The minimum space in megabytes that must be available on a disk for
            it to be used. If space on a disk falls below this threshold, it will be marked
            as bad. This applies to yarn.nodemanager.local-dirs and
            yarn.nodemanager.log-dirs.
        </description>
    </property>

    <property>
        <name>yarn.nodemanager.kill-escape.launch-command-line</name>
        <value>slider-agent,LLAP</value>
    </property>
    <property>
        <name>yarn.nodemanager.kill-escape.user</name>
        <value>hive</value>
    </property>



    <!-- Resource manager -->

    <property>
        <name>yarn.resourcemanager.bind-host</name>
        <value>0.0.0.0</value>
        <description>
            The actual address the server will bind to. If this optional address is
            set, the RPC and webapp servers will bind to this address and the port specified in
            yarn.resourcemanager.address and yarn.resourcemanager.webapp.address, respectively. This
            is most useful for making RM listen to all interfaces by setting to 0.0.0.0.
        </description>
    </property>


    <property>
        <name>yarn.resourcemanager.recovery.enabled</name>
        <value>true</value>
    </property>

    <property>
        <name>yarn.resourcemanager.max-completed-applications</name>
        <value>20000</value>
        <default>1000</default>
        <description>The maximum number of completed applications RM keeps.</description>
    </property>


    <property>
        <name>yarn.resourcemanager.monitor.capacity.preemption.max_ignored_over_capacity</name>
        <value>0.1</value>
    </property>
    <property>
        <name>yarn.resourcemanager.monitor.capacity.preemption.max_wait_before_kill</name>
        <value>1000</value>
    </property>
    <property>
        <name>yarn.resourcemanager.monitor.capacity.preemption.monitoring_interval</name>
        <value>1000</value>
    </property>
    <property>
        <name>yarn.resourcemanager.monitor.capacity.preemption.natural_termination_factor</name>
        <value>0.5</value>
    </property>
    <property>
        <name>yarn.resourcemanager.monitor.capacity.preemption.total_preemption_per_round</name>
        <value>1.0</value>
    </property>


    <property>
        <name>yarn.resourcemanager.proxy-user-privileges.enabled</name>
        <value>true</value>
        <description>If true, ResourceManager will have proxy-user privileges.
            Use case: In a secure cluster, YARN requires the user hdfs delegation-tokens to
            do localization and log-aggregation on behalf of the user. If this is set to true,
            ResourceManager is able to request new hdfs delegation tokens on behalf of
            the user. This is needed by long-running-service, because the hdfs tokens
            will eventually expire and YARN requires new valid tokens to do localization
            and log-aggregation. Note that to enable this use case, the corresponding
            HDFS NameNode has to configure ResourceManager as the proxy-user so that
            ResourceManager can itself ask for new tokens on behalf of the user when
            tokens are past their max-life-time.
        </description>
        <default>false</default>
    </property>
    <property>
        <name>yarn.resourcemanager.proxyuser.*.groups</name>
        <value></value>
    </property>
    <property>
        <name>yarn.resourcemanager.proxyuser.*.hosts</name>
        <value></value>
    </property>
    <property>
        <name>yarn.resourcemanager.proxyuser.*.users</name>
        <value></value>
    </property>



    <property>
        <name>yarn.resourcemanager.store.class</name>
        <value>org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore</value>
        <description>The class to use as the persistent store.

            If org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore
            is used, the store is implicitly fenced; meaning a single ResourceManager
            is able to use the store at any point in time. More details on this
            implicit fencing, along with setting up appropriate ACLs is discussed
            under yarn.resourcemanager.zk-state-store.root-node.acl.
        </description>
    </property>


    <property>
        <name>yarn.system-metrics-publisher.enabled</name>
        <value>true</value>
        <description>The setting that controls whether yarn system metrics is
            published on the Timeline service or not by RM And NM.
        </description>
    </property>


    <!-- Scheduler -->
    <property>
        <name>yarn.scheduler.capacity.ordering-policy.priority-utilization.underutilized-preemption.enabled</name>
        <value>true</value>
    </property>
    <property>
        <name>yarn.scheduler.maximum-allocation-mb</name>
        <value>{{ yarn_scheduler_max_memory }}</value>
        <description>The maximum allocation for every container request at the RM
            in MBs. Memory requests higher than this will throw an
            InvalidResourceRequestException.
        </description>
    </property>
    <property>
        <name>yarn.scheduler.maximum-allocation-vcores</name>
        <value>{{ yarn_scheduler_max_cpu_cores }}</value>
        <description>The maximum allocation for every container request at the RM
            in terms of virtual CPU cores. Requests higher than this will throw an
            InvalidResourceRequestException.
        </description>
    </property>
    <property>
        <name>yarn.scheduler.minimum-allocation-mb</name>
        <value>{{ yarn_scheduler_min_memory }}</value>
        <description>The minimum allocation for every container request at the RM
            in MBs. Memory requests lower than this will be set to the value of this
            property. Additionally, a node manager that is configured to have less memory
            than this value will be shut down by the resource manager.
        </description>
    </property>
    <property>
        <name>yarn.scheduler.minimum-allocation-vcores</name>
        <value>{{ yarn_scheduler_min_cpu_cores }}</value>
        <description>The minimum allocation for every container request at the RM
            in terms of virtual CPU cores. Requests lower than this will be set to the
            value of this property. Additionally, a node manager that is configured to
            have fewer virtual cores than this value will be shut down by the resource
            manager.
        </description>
    </property>


    <!-- Timeline server -->
    <property>
        <name>yarn.timeline-service.enabled</name>
        <value>true</value>
        <description>
            In the server side it indicates whether timeline service is enabled or not.
            And in the client side, users can enable it to indicate whether client wants
            to use timeline service. If its enabled in the client side along with
            security, then yarn client tries to fetch the delegation tokens for the
            timeline server.
        </description>
    </property>

    <property>
        <name>yarn.timeline-service.version</name>
        <value>1.5</value>
        <description>Indicate what is the current version of the running
            timeline service. For example, if "yarn.timeline-service.version" is 1.5,
            and "yarn.timeline-service.enabled" is true, it means the cluster will and
            should bring up the timeline service v.1.5 (and nothing else).
            On the client side, if the client uses the same version of timeline service,
            it should succeed. If the client chooses to use a smaller version in spite of this,
            then depending on how robust the compatibility story is between versions,
            the results may vary.
        </description>
    </property>



    <property>
        <name>yarn.timeline-service.ttl-enable</name>
        <value>true</value>
        <description>Enable age off of timeline store data.</description>
    </property>
    <property>
        <name>yarn.timeline-service.ttl-ms</name>
        <value>2592000000</value>
        <description>Time to live for timeline store data in milliseconds.</description>
    </property>


    <!-- timeline service ports and hosts -->
    <property>
        <name>yarn.timeline-service.address</name>
        <value>{{ yarn_timelineserver }}:{{ yarn_timelineserver_port2 }}</value>
        <description>This is default address for the timeline server to start the
            RPC server.</description>
    </property>
    <property>
        <name>yarn.timeline-service.webapp.address</name>
        <value>{{ yarn_timelineserver }}:{{ yarn_timelineserver_http_port }}</value>
        <description>The http address of the timeline service web application.</description>
    </property>
    <property>
        <name>yarn.timeline-service.webapp.https.address</name>
        <value>{{ yarn_timelineserver }}:{{ yarn_timelineserver_https_port }}</value>
        <description>The https address of the timeline service web application.</description>
    </property>
    <property>
        <name>yarn.timeline-service.bind-host</name>
        <value>0.0.0.0</value>
        <description>
            The actual address the server will bind to. If this optional address is
            set, the RPC and webapp servers will bind to this address and the port specified in
            yarn.timeline-service.address and yarn.timeline-service.webapp.address, respectively.
            This is most useful for making the service listen to all interfaces by setting to
            0.0.0.0.
        </description>
    </property>




    <!-- timeline service entity group -->
    <property>
        <name>yarn.timeline-service.store-class</name>
        <value>org.apache.hadoop.yarn.server.timeline.EntityGroupFSTimelineStore</value>
        <description>Store class name for timeline store.</description>
        <default>org.apache.hadoop.yarn.server.timeline.LeveldbTimelineStore</default>
    </property>
    <property>
        <name>yarn.timeline-service.entity-group-fs-store.active-dir</name>
        <value>{{ yarn_timeline_server_dir }}/active/</value>
        <description>HDFS path to store active application’s timeline data</description>
    </property>
    <property>
        <name>yarn.timeline-service.entity-group-fs-store.done-dir</name>
        <value>{{ yarn_timeline_server_dir }}/done/</value>
        <description>HDFS path to store done application’s timeline data</description>
    </property>
    <property>
        <name>yarn.timeline-service.entity-group-fs-store.retain-seconds</name>
        <value>2592000</value>
        <description>
            How long the ATS v1.5 entity group file system storage will keep an
            application's data in the done directory.
        </description>
    </property>
    <property>
        <name>yarn.timeline-service.entity-group-fs-store.summary-store</name>
        <value>org.apache.hadoop.yarn.server.timeline.RollingLevelDBTimelineStore</value>
        <description>Summary storage for ATS v1.5</description>
    </property>

    <!-- timeline service generic application history -->
    <property>
        <name>yarn.timeline-service.generic-application-history.enabled</name>
        <value>true</value>
    </property>
    <property>
        <name>yarn.timeline-service.generic-application-history.fs-history-store.uri</name>
        <value>{{ yarn_timeline_server_dir }}/generic-history</value>
    </property>
    <property>
        <name>yarn.timeline-service.generic-application-history.max-applications</name>
        <value>30000</value>
    </property>
    <property>
        <name>yarn.timeline-service.generic-application-history.store-class</name>
        <value>org.apache.hadoop.yarn.server.applicationhistoryservice.FileSystemApplicationHistoryStore</value>
    </property>


    <!-- timeline service state store -->
    <property>
        <name>yarn.timeline-service.recovery.enabled</name>
        <value>true</value>
        <description>Enable timeline server to recover state after starting. If
            true, then yarn.timeline-service.state-store-class must be specified.
        </description>
    </property>
    <property>
        <name>yarn.timeline-service.state-store-class</name>
        <value>org.apache.hadoop.yarn.server.timeline.recovery.LeveldbTimelineStateStore</value>
        <description>Store class name for timeline state store.</description>
    </property>
    <property>
        <name>yarn.timeline-service.leveldb-state-store.path</name>
        <value>{{ yarn_timeline_store }}</value>
        <description>Store file name for leveldb state store.</description>
        <default>${hadoop.tmp.dir}/yarn/timeline</default>
    </property>




    <property>
        <name>yarn.resourcemanager.zk-acl</name>
        <value>sasl:{{ yarn_resourcemanager_service_name }}:rwcda</value>
    </property>


    {% if yarn_high_availability %}

        <property>
            <name>yarn.resourcemanager.cluster-id</name>
            <value>YAK-YARN</value>
            <description>Name of the cluster. In a HA setting,
                this is used to ensure the RM participates in leader
                election for this cluster and ensures it does not affect
                other clusters
            </description>
        </property>

        <property>
            <name>yarn.resourcemanager.ha.enabled</name>
            <value>true</value>
        </property>

        <property>
            <name>yarn.resourcemanager.ha.rm-ids</name>
            <value>{{ groups['hostgroup_yarn_resource_managers'] | map('extract',hostvars, 'ansible_fqdn') | join(',') }}</value>
        </property>
        {% for resource_manager in groups['hostgroup_yarn_resource_managers'] %}
            {% set resource_manager_fqdn=hostvars[resource_manager].ansible_fqdn %}
            <property>
                <name>yarn.resourcemanager.hostname.{{ resource_manager_fqdn }}</name>
                <value>{{ resource_manager_fqdn }}</value>
            </property>

            <property>
                <name>yarn.resourcemanager.address.{{ resource_manager_fqdn }}</name>
                <value>{{ resource_manager_fqdn }}:{{ yarn_resourcemanager_rpc_port }}</value>
            </property>
            <property>
                <name>yarn.resourcemanager.scheduler.address.{{ resource_manager_fqdn }}</name>
                <value>{{ resource_manager_fqdn }}:{{ yarn_resourcemanager_scheduler_port }}</value>
            </property>

            <property>
                <name>yarn.resourcemanager.resource-tracker.address.{{ resource_manager_fqdn }}</name>
                <value>{{ resource_manager_fqdn }}:{{ yarn_resourcemanager_tracker_port }}</value>
            </property>

            <property>
                <name>yarn.resourcemanager.admin.address.{{ resource_manager_fqdn }}</name>
                <value>{{ resource_manager_fqdn }}:{{ yarn_resourcemanager_admin_port }}</value>
            </property>

            <property>
                <name>yarn.resourcemanager.webapp.address.{{ resource_manager_fqdn }}</name>
                <value>{{ resource_manager_fqdn }}:{{ yarn_resourcemanager_http_port }}</value>
            </property>


            <property>
                <name>yarn.resourcemanager.webapp.https.address.{{ resource_manager_fqdn }}</name>
                <value>{{ resource_manager_fqdn }}:{{ yarn_resourcemanager_https_port }}</value>
            </property>


        {% endfor %}

    {% else %}


        <property>
            <name>yarn.resourcemanager.ha.enabled</name>
            <value>false</value>
        </property>

        <property>
            <name>yarn.resourcemanager.hostname</name>
            <value>{{ yarn_nonha_primary_resourcemanager }}</value>
        </property>

        <property>
            <name>yarn.resourcemanager.address</name>
            <value>{{ yarn_nonha_primary_resourcemanager }}:{{ yarn_resourcemanager_rpc_port }}</value>
        </property>
        <property>
            <name>yarn.resourcemanager.scheduler.address</name>
            <value>{{ yarn_nonha_primary_resourcemanager }}:{{ yarn_resourcemanager_scheduler_port }}</value>
        </property>

        <property>
            <name>yarn.resourcemanager.resource-tracker.address</name>
            <value>{{ yarn_nonha_primary_resourcemanager }}:{{ yarn_resourcemanager_tracker_port }}</value>
        </property>

        <property>
            <name>yarn.resourcemanager.admin.address</name>
            <value>{{ yarn_nonha_primary_resourcemanager }}:{{ yarn_resourcemanager_admin_port }}</value>
        </property>

        <property>
            <name>yarn.resourcemanager.webapp.address</name>
            <value>{{ yarn_nonha_primary_resourcemanager }}:{{ yarn_resourcemanager_http_port }}</value>
        </property>

        <property>
            <name>yarn.resourcemanager.webapp.https.address</name>
            <value>{{ yarn_nonha_primary_resourcemanager }}:{{ yarn_resourcemanager_https_port }}</value>
        </property>

    {% endif %}





    {% if kerberos_authentication %}
        <!--    Kerberos-->
        <property>
            <name>yarn.nodemanager.principal</name>
            <value>{{ yarn_nodemanager_service_name }}/_HOST@{{ ipaserver_realm }}</value>
        </property>
        <property>
            <name>yarn.nodemanager.keytab</name>
            <value>{{ keytab_dir }}/{{ yarn_nodemanager_service_name }}.service.keytab</value>
        </property>
        <property>
            <name>yarn.nodemanager.webapp.spnego-principal</name>
            <value>HTTP/_HOST@{{ ipaserver_realm }}</value>
        </property>
        <property>
            <name>yarn.nodemanager.webapp.spnego-keytab-file</name>
            <value>{{ keytab_dir }}/spnego.service.keytab</value>
        </property>

        <property>
            <name>yarn.resourcemanager.principal</name>
            <value>{{ yarn_resourcemanager_service_name }}/_HOST@{{ ipaserver_realm }}</value>
        </property>
        <property>
            <name>yarn.resourcemanager.keytab</name>
            <value>{{ keytab_dir }}/{{ yarn_resourcemanager_service_name }}.service.keytab</value>
        </property>
        <property>
            <name>yarn.resourcemanager.webapp.spnego-principal</name>
            <value>HTTP/_HOST@{{ ipaserver_realm }}</value>
        </property>
        <property>
            <name>yarn.resourcemanager.webapp.spnego-keytab-file</name>
            <value>{{ keytab_dir }}/spnego.service.keytab</value>
        </property>

        <property>
            <name>yarn.timeline-service.http-authentication.kerberos.principal</name>
            <value>HTTP/_HOST@{{ ipaserver_realm }}</value>
        </property>
        <property>
            <name>yarn.timeline-service.http-authentication.kerberos.keytab</name>
            <value>{{ keytab_dir }}/spnego.service.keytab</value>
        </property>
        <property>
            <name>yarn.timeline-service.http-authentication.type</name>
            <value>kerberos</value>
        </property>
        <property>
            <name>yarn.timeline-service.principal</name>
            <value>{{ yarn_timelineserver_service_name }}/_HOST@{{ ipaserver_realm }}</value>
        </property>
        <property>
            <name>yarn.timeline-service.keytab</name>
            <value>{{ keytab_dir }}/{{ yarn_timelineserver_service_name }}.service.keytab</value>
        </property>

    {% else  %}

        <property>
            <name>yarn.timeline-service.http-authentication.simple.anonymous.allowed</name>
            <value>false</value>
            <description>
                Indicates if anonymous requests are allowed by the timeline server when using
                'simple' authentication.
            </description>
        </property>

    {%  endif %}


</configuration>
