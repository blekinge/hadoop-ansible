{{ ansible_managed | comment('xml') }}
<configuration>

    <property>
        <name>yarn.scheduler.capacity.maximum-am-resource-percent</name>
        <value>0.1</value>
        <description>Maximum percent of resources in the cluster which can be used to run application masters - controls
            number of concurrent active applications. Limits on each queue are directly proportional to their queue
            capacities and user limits. Specified as a float - ie 0.5 = 50%. Default is 10%. This can be set for all
            queues with yarn.scheduler.capacity.maximum-am-resource-percent and can also be overridden on a per queue
            basis by setting yarn.scheduler.capacity."queue-path".maximum-am-resource-percent
        </description>
    </property>

    <property>
        <name>yarn.scheduler.capacity.maximum-applications</name>
        <value>10000</value>
        <description>Maximum number of applications in the system which can be concurrently active both running and
            pending. Limits on each queue are directly proportional to their queue capacities and user limits. This is a
            hard limit and any applications submitted when this limit is reached will be rejected. Default is 10000.
            This can be set for all queues with yarn.scheduler.capacity.maximum-applications and can also be overridden
            on a per queue basis by setting yarn.scheduler.capacity."queue-path".maximum-applications. Integer value
            expected.
        </description>
    </property>

    <property>
        <name>yarn.scheduler.capacity.node-locality-delay</name>
        <value>40</value>
        <description>Number of missed scheduling opportunities after which the CapacityScheduler attempts to schedule
            rack-local containers. Typically, this should be set to number of nodes in the cluster. By default is
            setting approximately number of nodes in one rack which is 40. Positive integer value is expected.
        </description>
    </property>

    <!-- Queue Mapping based on User or Group, Application Name or user defined placement rules -->

    <property>
        <name>yarn.scheduler.capacity.queue-mappings</name>
        <value>{{ queue_mappings | default([]) | join(',') }}</value>
        <description>This configuration specifies the mapping of user or group to a specific queue. You can map a single
            user or a list of users to queues. Syntax: [u or g]:[name]:[queue_name][,next_mapping]*. Here, u or g
            indicates whether the mapping is for a user or group. The value is u for user and g for group. name
            indicates the user name or group name. To specify the user who has submitted the application, %user can be
            used. queue_name indicates the queue name for which the application has to be mapped. To specify queue name
            same as user name, %user can be used. To specify queue name same as the name of the primary group for which
            the user belongs to, %primary_group can be used.
        </description>
    </property>

    <property>
        <name>yarn.scheduler.queue-placement-rules.app-name</name>
        <description>This configuration specifies the mapping of application_name to a specific queue. You can map a
            single application or a list of applications to queues. Syntax: [app_name]:[queue_name][,next_mapping]*.
            Here, app_name indicates the application name you want to do the mapping. queue_name indicates the queue
            name for which the application has to be mapped. To specify the current application’s name as the app_name,
            %application can be used.
        </description>
    </property>

    <property>
        <name>yarn.scheduler.capacity.queue-mappings-override.enable</name>
        <value>false</value>
        <description>This function is used to specify whether the user specified queues can be overridden. This is a
            Boolean value and the default value is false.
        </description>
    </property>


    <property>
        <name>yarn.scheduler.capacity.resource-calculator</name>
        <value>org.apache.hadoop.yarn.util.resource.DominantResourceCalculator</value>
        <description>The ResourceCalculator implementation to be used to compare Resources in the scheduler. The default
            i.e. org.apache.hadoop.yarn.util.resource.DefaultResourceCalculator only uses Memory while
            DominantResourceCalculator uses Dominant-resource to compare multi-dimensional resources such as Memory, CPU
            etc. A Java ResourceCalculator class name is expected.
        </description>
    </property>


    <property>
        <name>yarn.scheduler.capacity.per-node-heartbeat.multiple-assignments-enabled</name>
        <description>Whether to allow multiple container assignments in one NodeManager heartbeat. Defaults to true.
        </description>
    </property>

    <property>
        <name>yarn.scheduler.capacity.per-node-heartbeat.maximum-container-assignments</name>
        <description>If multiple-assignments-enabled is true, the maximum amount of containers that can be assigned in
            one NodeManager heartbeat. Default value is 100, which limits the maximum number of container assignments
            per heartbeat to 100. Set this value to -1 will disable this limit.
        </description>
    </property>

    <property>
        <name>yarn.scheduler.capacity.per-node-heartbeat.maximum-offswitch-assignments</name>
        <description>If multiple-assignments-enabled is true, the maximum amount of off-switch containers that can be
            assigned in one NodeManager heartbeat. Defaults to 1, which represents only one off-switch allocation
            allowed in one heartbeat.
        </description>
    </property>

    <property>
        <name>yarn.scheduler.capacity.root.accessible-node-labels</name>
        <value>*</value>
    </property>

    {% for queue in capacity_queues %}
        <!-- {{ queue }} -->


        <!--  {{ queue }}: Resource Allocation -->

        <property>
            <name>yarn.scheduler.capacity.{{ queue.longname }}.capacity</name>
            <value>{{ queue.capacity | default(100) }}</value>
            <description>Queue capacity in percentage (%) as a float (e.g. 12.5) OR as absolute resource queue minimum
                capacity. The sum of capacities for all queues, at each level, must be equal to 100. However if absolute
                resource is configured, sum of absolute resources of child queues could be less than it’s parent
                absolute resource capacity. Applications in the queue may consume more resources than the queue’s
                capacity if there are free resources, providing elasticity.
            </description>
        </property>

        <property>
            <name>yarn.scheduler.capacity.{{ queue.longname }}.maximum-capacity</name>
            <value>{{ queue.max_capacity | default(100) }}</value>
            <description>Maximum queue capacity in percentage (%) as a float OR as absolute resource queue maximum
                capacity. This limits the elasticity for applications in the queue. 1) Value is between 0 and 100. 2)
                Admin needs to make sure absolute maximum capacity >= absolute capacity for each queue. Also, setting
                this value to -1 sets maximum capacity to 100%.
            </description>
        </property>

        <property>
            <name>yarn.scheduler.capacity.{{ queue.longname }}.minimum-user-limit-percent</name>
            <value>{{ queue.minimum_user_limit | default(25) }}</value>
            <description>Each queue enforces a limit on the percentage of resources allocated to a user at any given
                time, if there is demand for resources. The user limit can vary between a minimum and maximum value. The
                former (the minimum value) is set to this property value and the latter (the maximum value) depends on
                the number of users who have submitted applications. For e.g., suppose the value of this property is 25.
                If two users have submitted applications to a queue, no single user can use more than 50% of the queue
                resources. If a third user submits an application, no single user can use more than 33% of the queue
                resources. With 4 or more users, no user can use more than 25% of the queues resources. A value of 100
                implies no user limits are imposed. The default is 100. Value is specified as a integer.
            </description>
        </property>


        <property>
            <name>yarn.scheduler.capacity.{{ queue.longname }}.maximum-allocation-mb</name>
            <description>The per queue maximum limit of memory to allocate to each container request at the Resource
                Manager. This setting overrides the cluster configuration yarn.scheduler.maximum-allocation-mb. This
                value must be smaller than or equal to the cluster maximum.
            </description>
        </property>


        <property>
            <name>yarn.scheduler.capacity.{{ queue.longname }}.maximum-allocation-vcores</name>
            <description>The per queue maximum limit of virtual cores to allocate to each container request at the
                Resource Manager. This setting overrides the cluster configuration
                yarn.scheduler.maximum-allocation-vcores. This value must be smaller than or equal to the cluster
                maximum.
            </description>
        </property>


        <!-- {{ queue }}: Running and Pending Application Limits  -->
        <property>
            <name>yarn.scheduler.capacity.{{ queue.longname }}.maximum-applications</name>
            <description>Maximum number of applications in the system which can be concurrently active both running and
                pending. Limits on each queue are directly proportional to their queue capacities and user limits. This
                is a hard limit and any applications submitted when this limit is reached will be rejected. Default is
                10000. This can be set for all queues with yarn.scheduler.capacity.maximum-applications and can also be
                overridden on a per queue basis by setting yarn.scheduler.capacity."queue-path".maximum-applications.
                Integer value expected.
            </description>
        </property>

        <property>
            <name>yarn.scheduler.capacity.{{ queue.longname }}.maximum-am-resource-percent</name>
            <description>Maximum percent of resources in the cluster which can be used to run application masters -
                controls number of concurrent active applications. Limits on each queue are directly proportional to
                their queue capacities and user limits. Specified as a float - ie 0.5 = 50%. Default is 10%. This can be
                set for all queues with yarn.scheduler.capacity.maximum-am-resource-percent and can also be overridden
                on a per queue basis by setting yarn.scheduler.capacity."queue-path".maximum-am-resource-percent
            </description>
        </property>


        <!-- {{ queue }}:  Queue Administration & Permissions -->
        <property>
            <name>yarn.scheduler.capacity.{{ queue.longname }}.state</name>
            <value>{{ queue.state | default('RUNNING') }}</value>
            <description>The state of the queue. Can be one of RUNNING or STOPPED. If a queue is in STOPPED state, new
                applications cannot be submitted to itself or any of its child queues. Thus, if the root queue is
                STOPPED no applications can be submitted to the entire cluster. Existing applications continue to
                completion, thus the queue can be drained gracefully. Value is specified as Enumeration.
            </description>
        </property>


        <property>
            <name>yarn.scheduler.capacity.{{ queue.longname }}.acl_submit_applications</name>
            <value>{{ queue.submitters | default('') }}</value>
            <description>The ACL which controls who can submit applications to the given queue. If the given user/group
                has necessary ACLs on the given queue or one of the parent queues in the hierarchy they can submit
                applications. ACLs for this property are inherited from the parent queue if not specified.
            </description>
        </property>

        <property>
            <name>yarn.scheduler.capacity.{{ queue.longname }}.acl_administer_queue</name>
            <value>{{ queue.admins | default('') }}</value>
            <description>The ACL which controls who can administer applications on the given queue. If the given
                user/group has necessary ACLs on the given queue or one of the parent queues in the hierarchy they can
                administer applications. ACLs for this property are inherited from the parent queue if not specified.
            </description>
        </property>

        <!-- Note: An ACL is of the form user1,user2 space group1,group2. The special value of * implies anyone. The special value of space implies no one. The default is * for the root queue if not specified. -->


        <!-- {{ queue }}: Queue lifetime for applications -->
        <property>
            <name>yarn.scheduler.capacity.{{ queue.longname }}.maximum-application-lifetime</name>
            <description>Maximum lifetime (in seconds) of an application which is submitted to a queue. Any value less
                than or equal to zero will be considered as disabled. The default is -1. If positive value is configured
                then any application submitted to this queue will be killed after it exceeds the configured lifetime.
                User can also specify lifetime per application in application submission context. However, user lifetime
                will be overridden if it exceeds queue maximum lifetime. It is point-in-time configuration. Note: This
                feature can be set at any level in the queue hierarchy. Child queues will inherit their parent’s value
                unless overridden at the child level. A value of 0 means no max lifetime and will override a parent’s
                max lifetime. If this property is not set or is set to a negative number, then this queue’s max lifetime
                value will be inherited from it’s parent.
            </description>
        </property>

        <property>
            <name>yarn.scheduler.capacity.{{ queue.longname }}.default-application-lifetime</name>
            <description>Default lifetime (in seconds) of an application which is submitted to a queue. Any value less
                than or equal to zero will be considered as disabled. If the user has not submitted application with
                lifetime value then this value will be taken. It is point-in-time configuration. This feature can be set
                at any level in the queue hierarchy. Child queues will inherit their parent’s value unless overridden at
                the child level. If set to less than or equal to 0, the queue’s max value must also be unlimited.
                Default lifetime can’t exceed maximum lifetime.
            </description>
        </property>

        <!-- {{ queue }}: Priority -->

        <property>
            <name>yarn.scheduler.capacity.{{ queue.longname }}.default-application-priority</name>
            <value>{{ queue.priority | default(0) }}</value>
            <description>Defines default application priority in a leaf queue.</description>
        </property>



        <property>
            <name>yarn.scheduler.capacity.{{ queue.longname }}.disable_preemption</name>
            <description>This configuration can be set to true to selectively disable preemption of application
                containers submitted to a given queue. This property applies only when system wide preemption is enabled
                by configuring yarn.resourcemanager.scheduler.monitor.enable to true and
                yarn.resourcemanager.scheduler.monitor.policies to ProportionalCapacityPreemptionPolicy. If this
                property is not set for a queue, then the property value is inherited from the queue’s parent.
            </description>
            <default>false</default>
        </property>

        <property>
            <name>yarn.scheduler.capacity.{{ queue.longname }}.intra-queue-preemption.disable_preemption</name>
            <description>This configuration can be set to true to selectively disable intra-queue preemption of application containers submitted to a given queue. This property applies only when system wide preemption is enabled by configuring yarn.resourcemanager.scheduler.monitor.enable to true, yarn.resourcemanager.scheduler.monitor.policies to ProportionalCapacityPreemptionPolicy, and yarn.resourcemanager.monitor.capacity.preemption.intra-queue-preemption.enabled to true. If this property is not set for a queue, then the property value is inherited from the queue’s parent.
            </description>
            <default>false</default>
        </property>


        <property>
            <name>yarn.scheduler.capacity.{{ queue.longname }}.queues</name>
            <value>{{ queue.subqueues | default('') }}</value>
            <description>A given queue’s children can be defined with the configuration knob:
                yarn.scheduler.capacity."queue-path".queues. Children do not inherit properties directly from the parent
                unless otherwise noted.
            </description>
        </property>


        <property>
            <name>yarn.scheduler.capacity.{{ queue.longname }}.ordering-policy</name>
            <value>{{ queue.ordering | default('fifo') }}</value>
        </property>

        <property>
            <name>yarn.scheduler.capacity.{{ queue.longname }}.ordering-policy.fair.enable-size-based-weight</name>
            <value>{{ queue.size_based | default('false') }}</value>
        </property>
        <!-- End queue {{ queue.longname }} -->



    {% endfor %}


</configuration>
