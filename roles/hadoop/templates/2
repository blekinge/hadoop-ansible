
/usr/lib/jvm/jre-1.8.0-openjdk/bin/java
-Dproc_namenode
-Xmx1024m
-Dhdp.version=2.6.5.0-292
-Djava.net.preferIPv4Stack=true
-Dhdp.version=
-Djava.net.preferIPv4Stack=true
-Dhdp.version=
-Djava.net.preferIPv4Stack=true
-Dhadoop.log.dir=/var/log/hadoop/hdfs
-Dhadoop.log.file=hadoop.log
-Dhadoop.home.dir=/usr/hdp/2.6.5.0-292/hadoop
-Dhadoop.id.str=hdfs
-Dhadoop.root.logger=INFO,DRFA
-Djava.library.path=:/usr/hdp/2.6.5.0-292/hadoop/lib/native/Linux-amd64-64:/usr/lib/hadoop/lib/native/Linux-amd64-64:/usr/hdp/current/hadoop-client/lib/native/Linux-amd64-64:/usr/hdp/2.6.5.0-292/hadoop/lib/native
-Dhadoop.policy.file=hadoop-policy.xml
-Djava.net.preferIPv4Stack=true
-Dhdp.version=2.6.5.0-292
-Dhadoop.log.dir=/var/log/hadoop/hdfs
-Dhadoop.log.file=hadoop-hdfs-namenode-kac-hdfs-001.kach.sblokalnet.log
-Dhadoop.home.dir=/usr/hdp/2.6.5.0-292/hadoop
-Dhadoop.id.str=hdfs
-Dhadoop.root.logger=INFO,DRFA
-Djava.library.path=:/usr/hdp/2.6.5.0-292/hadoop/lib/native/Linux-amd64-64:/usr/lib/hadoop/lib/native/Linux-amd64-64:/usr/hdp/current/hadoop-client/lib/native/Linux-amd64-64:/usr/hdp/2.6.5.0-292/hadoop/lib/native:/usr/lib/hadoop/lib/native/Linux-amd64-64:/usr/hdp/current/hadoop-client/lib/native/Linux-amd64-64:/usr/hdp/2.6.5.0-292/hadoop/lib/native/Linux-amd64-64:/usr/lib/hadoop/lib/native/Linux-amd64-64:/usr/hdp/current/hadoop-client/lib/native/Linux-amd64-64:/usr/hdp/2.6.5.0-292/hadoop/lib/native
-Dhadoop.policy.file=hadoop-policy.xml
-Djava.net.preferIPv4Stack=true
-server
-XX:ErrorFile=/var/log/hadoop/hdfs/hs_err_pid%p.log
-XX:ParallelGCThreads=8
-XX:+UseConcMarkSweepGC
-XX:NewSize=512m
-XX:MaxNewSize=512m
-XX:CMSInitiatingOccupancyFraction=70
-XX:+UseCMSInitiatingOccupancyOnly
-Xms4096m
-Xmx4096m
-Dhadoop.security.logger=INFO,DRFAS
-Dhdfs.audit.logger=INFO,DRFAAUDIT
-XX:OnOutOfMemoryError="/usr/hdp/current/hadoop-hdfs-namenode/bin/kill-name-node"
-Dorg.mortbay.jetty.Request.maxFormContentSize=-1
-Dhadoop.security.logger=INFO,DRFAS
org.apache.hadoop.hdfs.server.namenode.NameNode
