---
#Flags if you just need to depend on this role to get variables

#If true, the hadoop binaries is installed
install_hadoop: true
# If true, the hadoop system is configured on this node
configure_hadoop: true
# If true, the systemd services for HDFS is installed
install_hdfs: true
# If true, the systemd services for YARN is installed
install_yarn: true
# If true, the systemd services for MapReduce is installed
install_mapreduce: true

# Should hdfs be configured for high availability
hdfs_high_availability: "{{ groups['hostgroup_hdfs_namenodes'] | length > 1 }}"

# Should yarn be configured for high availability
yarn_high_availability: "{{ groups['hostgroup_yarn_resource_managers'] | length > 1 }}"


#If true the cluster is set up for kerberos authentication. TODO test cluster without...
kerberos_authentication: true

# If true, every redeploy of name/data/journal nodes wipes HDFS
wipe_hdfs: false


# hadoop basic vars
# Set a local file here, if you do not want to download it dynamicallyl
hadoop_version: "3.3.0"
hadoop_download: http://ftp.download-by.net/apache/hadoop/common/hadoop-{{hadoop_version}}/hadoop-{{hadoop_version}}.tar.gz
hadoop_path: "/usr/local/hadoop"
hadoop_home: "{{ hadoop_path }}/hadoop-{{ hadoop_version }}"

current_hadoop_home: "{{ hadoop_path }}/current"

hadoop_config_path: "/etc/hadoop/conf"
hadoop_tmp: "/tmp/hadoop"
hadoop_log_dir: "/var/log/hadoop"
hadoop_pid_dir: "/var/run/hadoop"


#Loggers
hadoop_daemon_logger: "INFO,DRFA"
hdfs_daemon_logger: "{{hadoop_daemon_logger}}"
yarn_daemon_logger: "{{hadoop_daemon_logger}},EWMA"
mapreduce_daemon_logger: "{{hadoop_daemon_logger}}"

mapreduce_jobsummary_logger: "INFO,JSA"
yarn_resourcemanager_appsummary_logger: "DEBUG,RMSUMMARY"

hadoop_security_logger: "INFO,DRFAS"
hdfs_security_logger: "{{hadoop_security_logger}}"
mapreduce_security_logger: "{{hadoop_security_logger}}"

hadoop_audit_logger: "INFO"
hdfs_audit_logger: "{{hadoop_audit_logger}},DRFAAUDIT"
yarn_nodemanager_audit_logger: "{{hadoop_audit_logger}},NMAUDIT"
yarn_resourcemanager_audit_logger: "{{hadoop_audit_logger}},RMAUDIT"

# Memory and resources
hdfs_namenode_memory: "4096m"
hdfs_datanode_memory: "2048m"

mapreduce_map_java_memory: "1536m"
mapreduce_map_yarn_memory: 2048
mapreduce_reduce_java_memory: "1536m"
mapreduce_reduce_yarn_memory: 2048
mapreduce_am_java_memory: "1536m"
mapreduce_am_yarn_memory: 2048
mapreduce_task_sort_memory: 1024

yarn_nodemanager_available_cpu_cores: 1
yarn_nodemanager_available_memory: 1024

yarn_scheduler_min_cpu_cores: 1
yarn_scheduler_max_cpu_cores: 2
yarn_scheduler_min_memory: 1024
yarn_scheduler_max_memory: 3072

# Zookeeper

zookeeper_quorum: "{{ groups['hostgroup_zookeeper_servers'] | map('extract',hostvars, 'ansible_fqdn') | zip_longest([], fillvalue=zookeeper_port) | map('join', ':') | join(',') }}"


#HDFS
hadoop_dfs_name: "/hdfs/dfs/name"
hadoop_dfs_journal: "/hdfs/dfs/journal"
hdfs_checkpoints_dir: "/hadoop/hdfs/namesecondary"

hdfs_name: "KAC"

hdfs_data_dirs: []

hdfs_data_dirs_joined:  "{{ hdfs_data_dirs | join(',') }}"

#Primary namenode for non-HA setups
hdfs_nonha_primary_namenode: "{{ groups['hostgroup_hdfs_namenodes'] | map('extract',hostvars, 'ansible_fqdn') | first }}"


#YARN
yarn_timeline_store: "/hadoop/yarn/timeline"

yarn_timelineserver: "{{ groups['hostgroup_yarn_timeline_servers'] | map('extract',hostvars, 'ansible_fqdn') | first }}"
#Primary yarn node for non-HA setups
yarn_nonha_primary_resourcemanager: "{{ groups['hostgroup_yarn_resource_managers'] | map('extract',hostvars, 'ansible_fqdn') | first }}"

yarn_data_dirs: []
#  - /data/sdb1/yarn
#  - /data/sdc1/hdfs
#  - /data/sdd1/hdfs
#  - /data/sde1/hdfs
#  - /data/sdf1/hdfs
#  - /data/sdg1/hdfs

yarn_data_dirs_joined:  "{{ yarn_data_dirs | join(',') }}"

yarn_application_logs_dir: "{{ hadoop_log_dir }}/{{ yarn_user.name }}/application-logs/"
#yarn_user.user_logs: "/hadoop/yarn/

yarn_timeline_server_dir: '/history/timeline-server'

yarn_nodemanager_remote_app_log_dir: "/history/app-logs"

# Mapreduce
mapreduce_history_server_host: "{{ groups['hostgroup_mapreduce_history_servers'] | map('extract',hostvars, 'ansible_fqdn') | first }}"

mapreduce_create_path:
  - "/hadoop/mapreduce"

mapreduce_jobhistory_recovery_store_leveldb_path: "/hadoop/mapreduce/jhs"

mapreduce_jobhistory_dir: '/history/mr-history'
