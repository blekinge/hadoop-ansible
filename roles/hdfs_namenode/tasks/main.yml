---
- include: initialize_hdfs.yml
  when: wipe_hdfs | bool

- name: Create Name dir
  file:
    path: "{{hadoop_dfs_name}}"
    state: directory
    owner: "{{hdfs_user}}"
    group: "{{hdfs_user}}"


# Start both
- name: Start namenode process
  systemd:
    state: restarted
    daemon_reload: yes
    name: hdfs_namenode
    enabled: true

# Start the failover controllers
- name: Start namenode zkfc process
  systemd:
    state: restarted
    daemon_reload: yes
    name: hdfs_zkfc
    enabled: true


- name: create basic directory structure
  become: yes
  become_user: hdfs
  shell: |
    source /etc/profile.d/hadoop.sh
    source /etc/profile.d/kerberos.sh

    kinit -kt /etc/security/keytabs/hdfs.headless.keytab hdfs
    #TODO this should be configurable
    hdfs dfs -mkdir /tmp  \
        /system \
    	/history /history/app-logs  /history/spark-history    \
    	/history/mr-history /history/mr-history/done /history/mr-history/tmp \
    	/history/timeline-server /history/timeline-server/active /history/timeline-server/done /history/timeline-server/generic-history \
    	/user /user/abrsadm;

    hdfs dfs -chmod a+rwx /tmp;

    hdfs dfs -chmod o+rx /user;

    hdfs dfs -chmod -R o+rx /history;

    hdfs dfs -chmod -R a+rwx /history/mr-history/tmp;

    hdfs dfs -chown -R hdfs:hadoop /history;

    hdfs dfs -chown hdfs:hadoop /user;

    hdfs dfs -chown -R yarn:hadoop /history/app-logs /history/timeline-server;

    hdfs dfs -chown -R mapred:hadoop /history/mr-history;

    hdfs dfs -chown -R spark:hadoop /history/spark-history;

    hdfs dfs -chown -R abrsadm:abrsadm /user/abrsadm;
    kdestroy
  when: primary|bool
