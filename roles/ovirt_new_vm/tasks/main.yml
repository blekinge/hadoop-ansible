---
- name: setup vm
  delegate_to: localhost
  connection: local
  become: no
  throttle: 1
  block:
    # This DNS looks up the vm to determine it's configured IP address
    # Fun note: The VM does not have to exist yet, as long as it is in DNS
    - name: "Simple A record (IPV4 address) lookup for {{hostname}}"
      shell: "host {{hostname}} {{dns_server}} | grep '{{hostname}}' | cut -d' ' -f4 | cut -d'.' -f4"
      register: hostnumberblob


    # Save the hostnumber as variable; Hostnumber is the last block of the IP, as this is alike in all the networks
    - set_fact:
        hostnumber: "{{ hostnumberblob.stdout }}"

    - debug:
        var: hostnumber

    # The use of ovirt.ovirt before ovirt_auth is to check if the collection is correctly loaded
    - name: Obtain SSO token with using username/password credentials
      ovirt.ovirt.ovirt_auth:
        url: "https://{{ovirt_api_host}}/ovirt-engine/api"
        username: "{{ovirt_username}}"
        password: "{{ ovirt_password }}"

    - debug:
        msg: "Creating new vm {{shortname}}"

    # Previous task generated I(ovirt_auth) fact, which you can later use
    # in different modules as follows:

    - name: "create vm {{shortname}}"
      ovirt.ovirt.ovirt_vm:
        auth: "{{ ovirt_auth }}"
        state: present
        name: "{{shortname}}"
        template: "{{template}}"
        clone: yes
        cluster: "{{ovirt_cluster}}"

        memory: "{{memory}}"
        memory_guaranteed: "{{memory}}"
        cpu_sockets: "{{cpu_sockets}}"
        cpu_cores: "{{cpu_cores}}"
        comment: "{{comment | default(omit)}}"
        description: "{{description | default(omit)}}"

    - debug:
        msg: "Creating new disks for {{shortname}}"

    - name: "create extra disks"
      when: host_disks is defined
      loop: "{{ host_disks }}"
      register: attached_disks
      ovirt.ovirt.ovirt_disk:
        auth: "{{ ovirt_auth }}"
        activate: yes
        bootable: no
        content_type: data
        format: cow
        interface: "virtio_scsi"
        name: "{{shortname}}_{{item.name}}"
        shareable: no
        sparse: yes
        size: "{{item.size}}"
        state: present
        storage_domain: "VMs"
        vm_name: "{{shortname}}"
        wait: yes

    - name: "create {{nic1}} nic"
      ovirt.ovirt.ovirt_nic:
        auth: "{{ ovirt_auth }}"
        state: present
        vm: "{{shortname}}"
        name: nic1
        profile: "{{nic1}}"
        network: "{{nic1}}"
      register: nic1_added

    - name: "create {{nic2}} nic"
      ovirt.ovirt.ovirt_nic:
        auth: "{{ ovirt_auth }}"
        state: present
        vm: "{{shortname}}"
        name: nic2
        profile: "{{nic2}}"
        network: "{{nic2}}"
      register: nic2_added

    - debug:
        msg: "Starting {{shortname}}"

    - name: "Started vm {{shortname}}"
      ovirt.ovirt.ovirt_vm:
        auth: "{{ ovirt_auth }}"
        state: running
        name: "{{shortname}}"
        cluster: "{{ovirt_cluster}}"
    - name: setup network
      when: nic1_added.changed or nic2_added.changed
      block:
        # There is almost certainly a better way of doing this but this works
        - name: "ssh to host and change hostname"
          shell: "ssh -o 'StrictHostKeyChecking no'  root@{{new_vm_ip}} hostnamectl set-hostname {{hostname}}"

        - name: "ssh to host and setup network {{nic1}}"
          shell: "ssh -o 'StrictHostKeyChecking no' root@{{new_vm_ip}} nmcli con add type ethernet con-name {{nic1}} ifname ens3 ipv4.method manual ipv4.addresses {{nic1_subnet}}.{{hostnumber}}/24 ipv4.dns {{dns1}},{{dns2}} ipv4.dns-search {{dns_search_domains}} ipv4.gateway {{gateway}}"

        - name: "ssh to host and setup network {{nic2}}"
          shell: "ssh -o 'StrictHostKeyChecking no' root@{{new_vm_ip}} 'nmcli con add type ethernet con-name {{nic2}} ifname ens4 ipv4.method manual ipv4.addresses {{nic2_subnet}}.{{hostnumber}}/24 ipv4.routes \"{{nic2_routes}}\"'"

        - name: "ssh to host and remove old networks"
          shell: "ssh -o 'StrictHostKeyChecking no' root@{{new_vm_ip}} 'nmcli con del ens4; nmcli con del ens3;'"

        - name: "Reboot vm to activate new networks"
          ovirt.ovirt.ovirt_vm:
            auth: "{{ ovirt_auth }}"
            state: reboot
            name: "{{shortname}}"
            cluster: "{{ovirt_cluster}}"
            wait: yes


    - name: "Ensure vm is started"
      ovirt.ovirt.ovirt_vm:
        auth: "{{ ovirt_auth }}"
        state: running
        name: "{{shortname}}"
        cluster: "{{ovirt_cluster}}"
        wait: yes

  always:
    - name: Always revoke the SSO token
      ovirt_auth:
        state: absent
        ovirt_auth: "{{ ovirt_auth }}"

- name: "Wait 600 seconds for host to become reachable/usable"
  remote_user: root # Ensure that we do not try to login as a user that does not exist yet
  delegate_to: "{{hostname}}"
  become: no
  wait_for_connection:
    delay: 0 # Number of seconds to wait before starting to poll.
    timeout: 600 # Maximum number of seconds to wait for.
    sleep: 2 # Number of seconds to sleep between checks.
    connect_timeout: 5 # Maximum number of seconds to wait for a connection to happen before closing and retrying.


- name: "Try to ping to ensure networks are active"
  remote_user: root # Ensure that we do not try to login as a user that does not exist yet
  delegate_to: "{{hostname}}"
  become: no
  shell: ping wonky.statsbiblioteket.dk -c 5


- name: "Wait 600 seconds for host to become reachable/usable"
  remote_user: root # Ensure that we do not try to login as a user that does not exist yet
  become: no
  wait_for_connection:
    delay: 0 # Number of seconds to wait before starting to poll.
    timeout: 600 # Maximum number of seconds to wait for.
    sleep: 2 # Number of seconds to sleep between checks.
    connect_timeout: 5 # Maximum number of seconds to wait for a connection to happen before closing and retrying.


#TODO there might be an issue there. It seems the hosts are not available on their *.adm.yak2.net addresses until they have
# tried to ping wonky.statsbiblioteket.dk

- name: format+mount extra disks
  remote_user: root #TODO unclear if this takes effect or needs to be in task_format_disk.yml
  become: no
  include: task_format_disk.yml mount="{{item.mount}}" device="/dev/disk/by-id/scsi-0QEMU_QEMU_HARDDISK_{{(attached_disks.results[idx]['id'])[0:20]}}"
  when: host_disks is defined
  loop: "{{host_disks}}"
  loop_control:
    index_var: idx