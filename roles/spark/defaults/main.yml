---

# hadoop basic vars
# Set a local file here, if you do not want to download it dynamicallyl
spark_version: "3.0.1"
spark_download: "http://ftp.download-by.net/apache/spark/spark-{{spark_version}}/spark-{{spark_version}}-bin-hadoop3.2.tgz"
spark_path: "/usr/local/spark"
spark_home: "{{ spark_path }}/spark-{{spark_version}}-bin-hadoop3.2"

current_spark_home: "{{ spark_path }}/current"

spark_config_path: "/etc/spark/conf"
spark_tmp: "/tmp/spark"
spark_log_dir: "/var/log/spark"
spark_pid_dir: "/var/run/spark"

spark_history_server: "{{ groups['spark_history_servers'] | first }}"

#Place spark own jars first, to prevent errors with older hadoop paranamer jar. See https://issues.apache.org/jira/browse/SPARK-26819
spark_classpath_list:
  - "{{current_spark_home}}/jars/*"
  - "{{current_spark_home}}/yarn/*"
  - "{{current_hadoop_home}}/share/hadoop/common/*"
  - "{{current_hadoop_home}}/share/hadoop/common/lib/*"
#  - "{{current_hive_home}}/lib/*"
#  - "{{current_hadoop_home}}/share/hadoop/client/hadoop-client-api-{{hadoop_version}}.jar"
#  - "{{current_hadoop_home}}/share/hadoop/client/hadoop-client-runtime-{{hadoop_version}}.jar"
  - "{{current_hadoop_home}}/share/hadoop/hdfs/*"
  - "{{current_hadoop_home}}/share/hadoop/hdfs/lib/*"
  - "{{current_hadoop_home}}/share/hadoop/yarn/*"
  - "{{current_hadoop_home}}/share/hadoop/yarn/lib/*"


spark_classpath: "{{spark_classpath_list | join(':')}}"